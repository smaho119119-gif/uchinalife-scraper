name: Daily Database Backup

on:
  schedule:
    # æ¯Žæ—¥1æ™‚JSTï¼ˆUTC 16:00ï¼‰ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®1æ™‚é–“å¾Œï¼‰
    - cron: '0 16 * * *'
  
  # æ‰‹å‹•å®Ÿè¡Œã‚‚å¯èƒ½
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-22.04
    timeout-minutes: 30
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install supabase pandas
      
      - name: Create backup script
        run: |
          cat > backup.py << 'EOF'
          import os
          import json
          from datetime import datetime
          from supabase import create_client
          import pandas as pd
          
          # SupabaseæŽ¥ç¶š
          url = os.environ['SUPABASE_URL']
          key = os.environ['SUPABASE_ANON_KEY']
          supabase = create_client(url, key)
          
          # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
          os.makedirs('backups', exist_ok=True)
          
          # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
          timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
          
          print(f"ðŸ”„ Starting backup at {timestamp}")
          
          # å…¨ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
          print("ðŸ“Š Fetching all properties...")
          response = supabase.table('properties').select('*').execute()
          properties = response.data
          
          print(f"âœ… Found {len(properties)} properties")
          
          # JSONå½¢å¼ã§ä¿å­˜
          json_file = f'backups/properties_backup_{timestamp}.json'
          with open(json_file, 'w', encoding='utf-8') as f:
              json.dump(properties, f, ensure_ascii=False, indent=2)
          print(f"ðŸ’¾ Saved JSON: {json_file}")
          
          # CSVå½¢å¼ã§ä¿å­˜
          csv_file = f'backups/properties_backup_{timestamp}.csv'
          df = pd.DataFrame(properties)
          df.to_csv(csv_file, index=False, encoding='utf-8-sig')
          print(f"ðŸ’¾ Saved CSV: {csv_file}")
          
          # çµ±è¨ˆæƒ…å ±
          stats = {
              'timestamp': timestamp,
              'total_properties': len(properties),
              'categories': df['category'].value_counts().to_dict() if 'category' in df.columns else {},
              'active_properties': len(df[df['is_active'] == True]) if 'is_active' in df.columns else 0
          }
          
          stats_file = f'backups/backup_stats_{timestamp}.json'
          with open(stats_file, 'w', encoding='utf-8') as f:
              json.dump(stats, f, ensure_ascii=False, indent=2)
          print(f"ðŸ“ˆ Saved stats: {stats_file}")
          
          print("âœ… Backup completed successfully!")
          EOF
      
      - name: Run backup
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        run: |
          python backup.py
      
      - name: Upload backup as artifact
        uses: actions/upload-artifact@v4
        with:
          name: database-backup-${{ github.run_number }}
          path: backups/
          retention-days: 90  # 90æ—¥é–“ä¿å­˜
      
      - name: Commit backup to repository (optional)
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git add backups/ || true
          git commit -m "ðŸ”„ Auto backup $(date +'%Y-%m-%d %H:%M:%S')" || echo "No changes to commit"
          git push || echo "Nothing to push"
        continue-on-error: true
      
      - name: Notify on failure
        if: failure()
        run: |
          echo "::error::Backup failed. Check logs for details."
